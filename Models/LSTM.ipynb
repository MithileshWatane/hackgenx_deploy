{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrMXunUbdJLZ",
        "outputId": "4dd01206-d469-44a7-9f37-ed0131642149"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 0.1063 - mae: 0.2648 - val_loss: 0.0229 - val_mae: 0.1311\n",
            "Epoch 2/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0235 - mae: 0.1223 - val_loss: 0.0174 - val_mae: 0.1125\n",
            "Epoch 3/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0214 - mae: 0.1165 - val_loss: 0.0142 - val_mae: 0.0985\n",
            "Epoch 4/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0207 - mae: 0.1134 - val_loss: 0.0136 - val_mae: 0.0936\n",
            "Epoch 5/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0203 - mae: 0.1121 - val_loss: 0.0136 - val_mae: 0.0934\n",
            "Epoch 6/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0190 - mae: 0.1093 - val_loss: 0.0139 - val_mae: 0.0932\n",
            "Epoch 7/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0201 - mae: 0.1095 - val_loss: 0.0135 - val_mae: 0.0939\n",
            "Epoch 8/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0195 - mae: 0.1095 - val_loss: 0.0136 - val_mae: 0.0935\n",
            "Epoch 9/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0180 - mae: 0.1061 - val_loss: 0.0138 - val_mae: 0.0967\n",
            "Epoch 10/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0189 - mae: 0.1090 - val_loss: 0.0136 - val_mae: 0.0955\n",
            "Epoch 11/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0198 - mae: 0.1107 - val_loss: 0.0143 - val_mae: 0.0992\n",
            "Epoch 12/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0198 - mae: 0.1096 - val_loss: 0.0138 - val_mae: 0.0967\n",
            "Epoch 13/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0198 - mae: 0.1083 - val_loss: 0.0135 - val_mae: 0.0942\n",
            "Epoch 14/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0190 - mae: 0.1084 - val_loss: 0.0137 - val_mae: 0.0956\n",
            "Epoch 15/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0191 - mae: 0.1083 - val_loss: 0.0137 - val_mae: 0.0937\n",
            "Epoch 16/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0203 - mae: 0.1127 - val_loss: 0.0138 - val_mae: 0.0968\n",
            "Epoch 17/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0175 - mae: 0.1035 - val_loss: 0.0137 - val_mae: 0.0956\n",
            "Epoch 18/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0190 - mae: 0.1084 - val_loss: 0.0136 - val_mae: 0.0944\n",
            "Epoch 19/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0168 - mae: 0.1025 - val_loss: 0.0135 - val_mae: 0.0944\n",
            "Epoch 20/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0192 - mae: 0.1112 - val_loss: 0.0137 - val_mae: 0.0936\n",
            "Epoch 21/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0190 - mae: 0.1094 - val_loss: 0.0140 - val_mae: 0.0976\n",
            "Epoch 22/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0170 - mae: 0.1040 - val_loss: 0.0135 - val_mae: 0.0940\n",
            "Epoch 23/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0184 - mae: 0.1060 - val_loss: 0.0136 - val_mae: 0.0950\n",
            "Epoch 24/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0179 - mae: 0.1055 - val_loss: 0.0136 - val_mae: 0.0946\n",
            "Epoch 25/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0182 - mae: 0.1062 - val_loss: 0.0137 - val_mae: 0.0938\n",
            "Epoch 26/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0184 - mae: 0.1061 - val_loss: 0.0136 - val_mae: 0.0948\n",
            "Epoch 27/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0177 - mae: 0.1050 - val_loss: 0.0142 - val_mae: 0.0982\n",
            "Epoch 28/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0181 - mae: 0.1065 - val_loss: 0.0140 - val_mae: 0.0972\n",
            "Epoch 29/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0187 - mae: 0.1101 - val_loss: 0.0135 - val_mae: 0.0937\n",
            "Epoch 30/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0189 - mae: 0.1089 - val_loss: 0.0136 - val_mae: 0.0936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Model training complete and saved!\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step\n",
            "ğŸ“Š Predicted Waiting Time: 161.4200897216797 minutes\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "import joblib\n",
        "\n",
        "# ==============================\n",
        "# 1. LOAD DATA\n",
        "# ==============================\n",
        "\n",
        "data = pd.read_csv(\"hospital_queue.csv\")\n",
        "\n",
        "# Use only required columns\n",
        "features = data[['queue_length', 'service_time']]\n",
        "target = data['waiting_time']\n",
        "\n",
        "# ==============================\n",
        "# 2. NORMALIZE DATA\n",
        "# ==============================\n",
        "\n",
        "feature_scaler = MinMaxScaler()\n",
        "target_scaler = MinMaxScaler()\n",
        "\n",
        "features_scaled = feature_scaler.fit_transform(features)\n",
        "target_scaled = target_scaler.fit_transform(target.values.reshape(-1, 1))\n",
        "\n",
        "# Save scalers\n",
        "joblib.dump(feature_scaler, \"feature_scaler.save\")\n",
        "joblib.dump(target_scaler, \"target_scaler.save\")\n",
        "\n",
        "# ==============================\n",
        "# 3. CREATE SEQUENCES FOR LSTM\n",
        "# ==============================\n",
        "\n",
        "TIME_STEPS = 5\n",
        "\n",
        "def create_sequences(X, y, time_steps=5):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - time_steps):\n",
        "        Xs.append(X[i:(i + time_steps)])\n",
        "        ys.append(y[i + time_steps])\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "X, y = create_sequences(features_scaled, target_scaled, TIME_STEPS)\n",
        "\n",
        "# ==============================\n",
        "# 4. BUILD LSTM MODEL\n",
        "# ==============================\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(LSTM(64, return_sequences=True, input_shape=(TIME_STEPS, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(32))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"mse\",\n",
        "    metrics=[\"mae\"]\n",
        ")\n",
        "\n",
        "# ==============================\n",
        "# 5. TRAIN MODEL\n",
        "# ==============================\n",
        "\n",
        "history = model.fit(\n",
        "    X, y,\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ==============================\n",
        "# 6. SAVE MODEL\n",
        "# ==============================\n",
        "\n",
        "model.save(\"waiting_time_lstm_model.h5\")\n",
        "\n",
        "print(\"âœ… Model training complete and saved!\")\n",
        "\n",
        "# ==============================\n",
        "# 7. TEST PREDICTION (OPTIONAL)\n",
        "# ==============================\n",
        "\n",
        "# Take last sequence\n",
        "last_sequence = X[-1].reshape(1, TIME_STEPS, 2)\n",
        "\n",
        "pred_scaled = model.predict(last_sequence)\n",
        "\n",
        "# Convert back to original value\n",
        "pred = target_scaler.inverse_transform(pred_scaled)\n",
        "\n",
        "print(\"ğŸ“Š Predicted Waiting Time:\", float(pred[0][0]), \"minutes\")"
      ]
    }
  ]
}